#!/usr/bin/env python3
"""
Send a test email with the new light-mode design using mock data.
"""

import yaml
import os
from render import EmailRenderer
from send import ResendClient
from datetime import datetime
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Mock data for testing
mock_papers = [
    {
        'arxiv_id': '2508.10333v1',
        'title': 'ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver',
        'abstract': 'Recent advances in Vision-Language-Action (VLA) models have enabled robotic agents to integrate multimodal understanding with action execution. This paper introduces ReconVLA, a model that improves visual grounding for manipulation by using a reconstructive objective.',
        'categories': ['cs.RO', 'cs.CV'],
        'version': 1,
        'keep': True,
        'relevance_score': 85,
        'buckets': ['VLA / LLM-in-the-Loop', 'Perception for Manipulation'],
        'why_it_matters': 'For our dual-arm mobile manipulator operating in complex human environments, accurately identifying and focusing on the correct object is critical. This paper offers a concrete method to improve the visual grounding of our VLA-based policies.',
        'summary': 'This paper introduces ReconVLA, a Vision-Language-Action model that improves visual grounding for manipulation by using a reconstructive objective. The core method involves a diffusion transformer that reconstructs the target gaze region of an image from the VLA visual outputs.',
        'code_urls': ['https://github.com/zionchow/ReconVLA'],
        'dataset_urls': [],
        'arxiv_link': 'https://arxiv.org/abs/2508.10333',
        'pdf_link': 'https://arxiv.org/pdf/2508.10333.pdf',
        'in_top_picks': True
    },
    {
        'arxiv_id': '2508.10511v1',
        'title': 'KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection',
        'abstract': 'Learning robot policies that capture multimodality in the training data has been a long-standing open challenge for behavior cloning. This paper introduces KDPE, a method using Kernel Density Estimation to filter and select the best trajectories generated by a Diffusion Policy.',
        'categories': ['cs.RO'],
        'version': 1,
        'keep': True,
        'relevance_score': 70,
        'buckets': ['Imitation / Diffusion / RL', 'Safety & Reliability'],
        'why_it_matters': 'Diffusion policies are a key area of interest for us. This paper addresses a major weakness: their stochasticity and tendency to learn outliers, which can lead to unsafe or failed actions.',
        'summary': 'This paper introduces KDPE, a method using Kernel Density Estimation to filter and select the best trajectories generated by a Diffusion Policy. This improves the policy reliability by rejecting stochastic or out-of-distribution outputs.',
        'code_urls': ['https://hsp-iit.github.io/KDPE/'],
        'dataset_urls': [],
        'arxiv_link': 'https://arxiv.org/abs/2508.10511',
        'pdf_link': 'https://arxiv.org/pdf/2508.10511.pdf',
        'in_top_picks': True
    },
    {
        'arxiv_id': '2508.10399v1',
        'title': 'Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning',
        'abstract': 'Embodied AI aims to develop intelligent systems with physical forms capable of perceiving, decision-making, acting, and learning in real-world environments. This survey provides a comprehensive overview of how large models are being used to advance embodied AI.',
        'categories': ['cs.RO'],
        'version': 1,
        'keep': True,
        'relevance_score': 70,
        'buckets': ['VLA / LLM-in-the-Loop', 'Imitation / Diffusion / RL'],
        'why_it_matters': 'As a comprehensive survey, this paper is a critical resource for understanding the current landscape of large models in robotics. It helps our team quickly get up to speed on state-of-the-art approaches.',
        'summary': 'This survey provides a comprehensive overview of how large models are being used to advance embodied AI, focusing on decision-making paradigms like Vision-Language-Action (VLA) models and learning methodologies such as imitation and reinforcement learning.',
        'code_urls': [],
        'dataset_urls': [],
        'arxiv_link': 'https://arxiv.org/abs/2508.10399',
        'pdf_link': 'https://arxiv.org/pdf/2508.10399.pdf',
        'in_top_picks': True
    },
    {
        'arxiv_id': '2508.10416v1',
        'title': 'CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model',
        'abstract': 'Existing vision-and-language navigation models often deviate from the correct trajectory when executing instructions. This paper introduces Self-correction Flywheel, a post-training paradigm that improves VLA navigation models.',
        'categories': ['cs.RO', 'cs.AI', 'cs.CL', 'cs.CV'],
        'version': 1,
        'keep': True,
        'relevance_score': 65,
        'buckets': ['VLA / LLM-in-the-Loop', 'Navigation & Mapping', 'Safety & Reliability'],
        'why_it_matters': 'The self-correction flywheel concept is a potentially powerful and data-efficient way to improve model robustness. This paradigm could be adapted from navigation to our manipulation tasks.',
        'summary': 'This paper introduces Self-correction Flywheel, a post-training paradigm that improves VLA navigation models by using their own error trajectories to automatically generate self-correction data for fine-tuning.',
        'code_urls': [],
        'dataset_urls': [],
        'arxiv_link': 'https://arxiv.org/abs/2508.10416',
        'pdf_link': 'https://arxiv.org/pdf/2508.10416.pdf',
        'in_top_picks': False
    },
    {
        'arxiv_id': '2508.10398v1',
        'title': 'Super LiDAR Reflectance for Robotic Perception',
        'abstract': 'Conventionally, human intuition often defines vision as a modality of passive optical sensing, while active optical sensing is typically regarded as mechanical. This paper introduces a framework for generating dense LiDAR reflectance images.',
        'categories': ['cs.RO'],
        'version': 1,
        'keep': True,
        'relevance_score': 55,
        'buckets': ['Perception for Manipulation', 'Navigation & Mapping', 'Hardware & Mechatronics'],
        'why_it_matters': 'This method could enable our mobile manipulator to use lower-cost LiDARs for robust, illumination-invariant 3D perception. Denser point clouds would improve object detection and scene understanding.',
        'summary': 'This paper introduces a framework for generating dense LiDAR reflectance images from sparse data, specifically targeting low-cost, non-repeating scanning LiDARs.',
        'code_urls': [],
        'dataset_urls': [],
        'arxiv_link': 'https://arxiv.org/abs/2508.10398',
        'pdf_link': 'https://arxiv.org/pdf/2508.10398.pdf',
        'in_top_picks': False
    }
]

# Load config
with open('config.yaml', 'r') as f:
    config = yaml.safe_load(f)

# Create renderer and email client
renderer = EmailRenderer(config)
email_client = ResendClient(
    api_key=os.getenv('RESEND_API_KEY'),
    config=config
)

# Organize papers
top_picks = [p for p in mock_papers if p.get('in_top_picks', False)]
buckets = {
    'VLA / LLM-in-the-Loop': [p for p in mock_papers if 'VLA / LLM-in-the-Loop' in p.get('buckets', [])],
    'Imitation / Diffusion / RL': [p for p in mock_papers if 'Imitation / Diffusion / RL' in p.get('buckets', [])],
    'Perception for Manipulation': [p for p in mock_papers if 'Perception for Manipulation' in p.get('buckets', [])],
    'Safety & Reliability': [p for p in mock_papers if 'Safety & Reliability' in p.get('buckets', [])],
    'Navigation & Mapping': [p for p in mock_papers if 'Navigation & Mapping' in p.get('buckets', [])],
    'Hardware & Mechatronics': [p for p in mock_papers if 'Hardware & Mechatronics' in p.get('buckets', [])]
}
also_noteworthy = []
filtered_out = []

# Render email
html = renderer.render(
    top_picks=top_picks,
    buckets=buckets,
    also_noteworthy=also_noteworthy,
    filtered_out=filtered_out,
    metadata={'total_papers': len(mock_papers)}
)

# Send email
subject = f"Droyd Daily Robotics Digest - {datetime.now().strftime('%a, %b %d')} [Light Mode Preview]"
recipients = config['digest']['recipients']

success = email_client.send_digest(
    recipients=recipients,
    subject=subject,
    html_content=html
)

if success:
    print(f"‚úÖ Beautiful light-mode email sent successfully to {recipients}")
    print("üé® The email features:")
    print("   ‚Ä¢ Clean white background with subtle shadows")
    print("   ‚Ä¢ Beautiful gradient header (purple to indigo)")
    print("   ‚Ä¢ Modern card design with rounded corners")
    print("   ‚Ä¢ Colorful badges for different research areas")
    print("   ‚Ä¢ Elegant typography and spacing")
else:
    print("‚ùå Failed to send email")

# Also save to file for preview
with open('light_mode_preview.html', 'w') as f:
    f.write(html)

print("üìÑ Preview saved to: light_mode_preview.html")
